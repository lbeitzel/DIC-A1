{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Predicitve_Analytics.py\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()\n",
    "training_ind = np.random.rand(len(data)) <= 0.8\n",
    "training = data[training_ind]\n",
    "testing = data[~training_ind]\n",
    "X_train = training[0:32734, 0:48]\n",
    "Y_train = training[0:32734, 48:49]\n",
    "Y_train = Y_train.flatten()\n",
    "X_test = testing[0:8222, 0:48]\n",
    "Y_test = testing[0:8222, 48:49]\n",
    "Y_test = Y_test.flatten()\n",
    "\n",
    "#k = 5\n",
    "#print(KNN(X_train, X_test, Y_train, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-ac7e19be4af7>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ac7e19be4af7>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    #display(B)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([0,1,1,2])\n",
    "y_pred = np.array([0,1,1,1])\n",
    "A = ConfusionMatrix(y_true,y_pred)\n",
    "B = np.array([])\n",
    "#display(A[0,:])\n",
    "#display(np.sum(A[0,:]))\n",
    "display(y_true.shape[0])\n",
    "display(y_true.shape[0]-1)\n",
    "acct = A[0,0]/(np.sum(A[0,:])\n",
    "#for i in range(y_true.shape[0]-1):\n",
    "#    acct = A[i,i]/(np.sum(A[i,:]))\n",
    "#    display(acct)\n",
    "#    B = np.append(B,acct)\n",
    "#display(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0,1,1,2])\n",
    "y_pred = np.array([0,1,1,1])\n",
    "A = ConfusionMatrix(y_true,y_pred)\n",
    "B = np.array([])\n",
    "display(A.shape)\n",
    "for i in range(A.shape[0]):\n",
    "    display(i)\n",
    "    sumRow = (np.sum(A[i,:]))\n",
    "    if sumRow == 0:\n",
    "        sumRow = 1\n",
    "    acct = A[i,i]/sumRow\n",
    "    B = np.append(B,acct)\n",
    "mean = np.mean(B)\n",
    "display(B, mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Recall(y_true,y_pred))\n",
    "display(Precision(y_true,y_pred))\n",
    "display(Accuracy(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_true,y_pred):\n",
    "    A = ConfusionMatrix(y_true,y_pred)\n",
    "    trueVal = np.sum(A.diagonal())\n",
    "    total = np.sum(A)\n",
    "    return trueVal/total\n",
    "    \n",
    "    \"\"\"\n",
    "    :type y_true: numpy.ndarray\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :rtype: float\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(y_true,y_pred):\n",
    "    A = ConfusionMatrix(y_true,y_pred)\n",
    "    B = np.array([])\n",
    "    for i in range(A.shape[0]):\n",
    "        sumCol = (np.sum(A[:,i]))\n",
    "        if sumCol == 0:\n",
    "            sumCol = 1\n",
    "        acct = A[i,i]/sumCol\n",
    "        B = np.append(B,acct)\n",
    "    mean = np.mean(B)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(y_true,y_pred):\n",
    "    A = ConfusionMatrix(y_true,y_pred)\n",
    "    B = np.array([])\n",
    "    for i in range(A.shape[0]):\n",
    "        sumRow = (np.sum(A[i,:]))\n",
    "        if sumRow == 0:\n",
    "            sumRow = 1\n",
    "        acct = A[i,i]/sumRow\n",
    "        B = np.append(B,acct)\n",
    "    mean = np.mean(B)\n",
    "    return mean\n",
    "    \"\"\"\n",
    "    :type y_true: numpy.ndarray\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :rtype: float\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WCSS(Clusters):\n",
    "    \"\"\"\n",
    "    :Clusters List[numpy.ndarray]\n",
    "    :rtype: float\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_true,y_pred):\n",
    "    numClass = np.unique(y_true).shape[0]\n",
    "    C = y_true*numClass + y_pred\n",
    "    C = np.bincount(C)\n",
    "    C = np.pad(C, (0, numClass**2-C.shape[0]), 'constant')\n",
    "    D = C.reshape(numClass,numClass)\n",
    "    return np.transpose(D)\n",
    "    \"\"\"\n",
    "    :type y_true: numpy.ndarray\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :rtype: float\n",
    "    \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0,1,1,2])\n",
    "y_pred = np.array([0,1,1,1])\n",
    "#display(y_true)\n",
    "#display(np.unique(y_true))\n",
    "numClass = np.unique(y_true).shape[0]\n",
    "#display(numClass)\n",
    "C = y_true*numClass + y_pred\n",
    "C = np.bincount(C)\n",
    "C = np.pad(C, (0, numClass**2-C.shape[0]), 'constant')\n",
    "#display(C)\n",
    "D = C.reshape(numClass,numClass)\n",
    "#display(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training[0:800, 0:20]\n",
    "Y_train = training[0:800, 48:49]\n",
    "Y_train = Y_train.flatten()\n",
    "X_test = testing[0:200, 0:20]\n",
    "Y_test = testing[0:200, 48:49]\n",
    "Y_test = Y_test.flatten()\n",
    "k = 10\n",
    "\n",
    "y_pred = np.array([])\n",
    "for point in X_test:\n",
    "        dist = np.array([])\n",
    "        for row in X_train:\n",
    "            d = np.sqrt(np.sum((point-row)**2))\n",
    "            dist = np.append(dist, d)\n",
    "        attach = np.vstack((dist, Y_train))\n",
    "        sortDist = attach[:,attach[0].argsort()]\n",
    "        topK = sortDist[:,0:k]\n",
    "        topK = topK[1].astype(int)\n",
    "        counts = np.bincount(topK)\n",
    "        majority = np.argmax(counts)\n",
    "        y_pred = np.append(y_pred,majority)\n",
    "        \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_true = Y_test\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array([[[1,4,5,6],[7,2,5,5],[3,1,4,2]],[[3,5,6,5],[5,2,7,3],[8,6,1,2]]])\n",
    "display(mat)\n",
    "#mat_sort = mat[mat[:,2].argsort()[::-1]]\n",
    "#mat = mat[np.arange(len(mat))[:,np.newaxis]]\n",
    "#m = mat.view(dtype=[('', mat.dtype)]*mat.shape[-1])\n",
    "#m.sort(axis=1)\n",
    "#mat\n",
    "I, J, K = np.ogrid[tuple(map(slice, mat.shape))]\n",
    "# I, J, K are the identity indices in the sense that (A == A[I, J, K]).all()\n",
    "newK = np.argsort(mat[0],axis=-1) # first axis of A[1] is second axis of A\n",
    "display(mat[I, J, newK])\n",
    "#display(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,4,5,6,7],[7,2,5,5,3],[3,1,4,2,7],[6,2,4,2,7]])\n",
    "#A = np.transpose(A)\n",
    "B = np.array([[3,5,6,5,2],[5,2,7,3,1],[8,6,1,2,2],[5,2,2,2,5]])\n",
    "#B = np.transpose(B)\n",
    "C = np.array([A, B])\n",
    "I, J, K = np.ogrid[tuple(map(slice, C.shape))]\n",
    "# I, J, K are the identity indices in the sense that (A == A[I, J, K]).all()\n",
    "newK = np.argsort(C[0],axis=-1) # first axis of A[1] is second axis of A\n",
    "display(C[I, J, newK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3],[3,1,6]])\n",
    "display(A)\n",
    "B = np.array([[2,7,2],[8,2,4]])\n",
    "display(B)\n",
    "C = np.true_divide(A,B)\n",
    "display(C)\n",
    "mini = np.min(A,axis = 0)\n",
    "maxi = np.max(A,axis = 0)\n",
    "#display(mini)\n",
    "min2 = np.tile(mini, (A.shape[0],1))\n",
    "#display(min2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1],\n",
       "       [4, 5, 1],\n",
       "       [2, 3, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1],\n",
       "       [2, 1, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 1]],\n",
       "\n",
       "       [[2, 1, 2]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[2, 4, 2],\n",
       "        [5, 7, 2],\n",
       "        [3, 5, 2]],\n",
       "\n",
       "       [[3, 3, 3],\n",
       "        [6, 6, 3],\n",
       "        [4, 4, 3]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8, 14, 10],\n",
       "       [ 9, 15, 11]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = np.array([[1,2,1],[4,5,1],[2,3,1]])\n",
    "display(A)\n",
    "B = np.array([[1,2,1],[2,1,2]])\n",
    "display(B)\n",
    "B_new = B[:,np.newaxis]\n",
    "display(B_new)\n",
    "C = A+B_new\n",
    "display(C)\n",
    "display(B_new.shape)\n",
    "C = np.sum(C,axis=2)\n",
    "display(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(X_train.shape, X_test.shape)\n",
    "X_min = np.min(X_train,axis = 0)\n",
    "X_max = np.max(X_train,axis = 0)\n",
    "X_minMat = np.tile(X_min, (X_train.shape[0],1))\n",
    "X_maxMat = np.tile(X_max, (X_train.shape[0],1))\n",
    "X_train_normal = np.true_divide(X_train-X_minMat, X_maxMat-X_minMat)\n",
    "\n",
    "\n",
    "X_min2 = np.min(X_test,axis = 0)\n",
    "X_max2 = np.max(X_test,axis = 0)\n",
    "X_minMat2 = np.tile(X_min2, (X_test.shape[0],1))\n",
    "X_maxMat2 = np.tile(X_max2, (X_test.shape[0],1))\n",
    "X_test_normal = np.true_divide(X_test-X_minMat2, X_maxMat2-X_minMat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.56154869, 4.62418325, 4.30597545, ..., 4.59311981, 4.52319435,\n",
       "        4.40996669],\n",
       "       [4.58427501, 4.64865525, 4.32998554, ..., 4.61786109, 4.54721027,\n",
       "        4.42933797],\n",
       "       [4.46773207, 4.53164515, 4.21312257, ..., 4.50045221, 4.43044681,\n",
       "        4.31401073],\n",
       "       ...,\n",
       "       [5.11803095, 5.18198247, 4.86339949, ..., 5.15119867, 5.08058677,\n",
       "        4.9634763 ],\n",
       "       [3.176862  , 3.24036528, 2.92186272, ..., 3.20974969, 3.13915709,\n",
       "        3.0250442 ],\n",
       "       [4.15079006, 4.21377148, 3.89544712, ..., 4.18283608, 4.1126884 ,\n",
       "        3.99886568]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(200, 800)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 3.78733215,  3.86520559,  3.86680905, ...,  5.99440856,\n",
       "          5.9983311 ,  6.0059101 ],\n",
       "        [ 3.81192134,  3.88705305,  3.88706154, ...,  6.01582054,\n",
       "          6.01976314,  6.02736625],\n",
       "        [ 3.6944891 ,  3.77077795,  3.77234789, ...,  5.89977181,\n",
       "          5.90370438,  5.91128966],\n",
       "        ...,\n",
       "        [ 4.34523964,  4.42092601,  4.42122706, ...,  6.54983161,\n",
       "          6.55377285,  6.56137866],\n",
       "        [ 2.40386772,  2.48023586,  2.48183471, ...,  4.60944312,\n",
       "          4.6133699 ,  4.62095523],\n",
       "        [ 3.37697442,  3.45429189,  3.45589164, ...,  5.58350024,\n",
       "          5.58742548,  5.5950075 ]],\n",
       "\n",
       "       [[10.        ,  3.        ,  3.        , ...,  4.        ,\n",
       "          4.        ,  4.        ],\n",
       "        [10.        ,  7.        ,  3.        , ...,  4.        ,\n",
       "          4.        ,  4.        ],\n",
       "        [10.        ,  3.        ,  3.        , ...,  4.        ,\n",
       "          4.        ,  4.        ],\n",
       "        ...,\n",
       "        [10.        ,  3.        ,  7.        , ...,  4.        ,\n",
       "          4.        ,  4.        ],\n",
       "        [10.        ,  3.        ,  3.        , ...,  4.        ,\n",
       "          4.        ,  4.        ],\n",
       "        [10.        ,  3.        ,  3.        , ...,  4.        ,\n",
       "          4.        ,  4.        ]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[10.,  3.,  3.,  7.,  7.,  7.],\n",
       "       [10.,  7.,  3.,  3.,  7.,  7.],\n",
       "       [10.,  3.,  3.,  7.,  7.,  7.],\n",
       "       ...,\n",
       "       [10.,  3.,  7.,  3.,  7.,  7.],\n",
       "       [10.,  3.,  3.,  7.,  7.,  7.],\n",
       "       [10.,  3.,  3.,  7.,  7.,  7.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[10,  3,  3,  7,  7,  7],\n",
       "       [10,  7,  3,  3,  7,  7],\n",
       "       [10,  3,  3,  7,  7,  7],\n",
       "       ...,\n",
       "       [10,  3,  7,  3,  7,  7],\n",
       "       [10,  3,  3,  7,  7,  7],\n",
       "       [10,  3,  3,  7,  7,  7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.065"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#EUCLIDEAN\n",
    "X_train = training[0:800, 0:20]\n",
    "Y_train = training[0:800, 48:49]\n",
    "Y_train = Y_train.flatten()\n",
    "X_test = testing[0:200, 0:20]\n",
    "Y_test = testing[0:200, 48:49]\n",
    "Y_test = Y_test.flatten()\n",
    "X_train_normal = X_train\n",
    "X_test_normal = X_test\n",
    "\n",
    "A2 = np.square(X_train_normal)\n",
    "B2 = np.square(X_test_normal)\n",
    "B2_new = B2[:,np.newaxis]\n",
    "A2andB2 = A2+B2_new\n",
    "A2andB2 = np.sum(A2andB2,axis=2)\n",
    "X_train_normalnew = np.transpose(X_train_normal)\n",
    "AB2 = np.multiply(2, np.dot(X_test_normal,X_train_normalnew))\n",
    "Euclidean = np.sqrt(A2andB2 + AB2)\n",
    "display(Euclidean, Euclidean.shape)\n",
    "\n",
    "k = 6\n",
    "trueLabel = np.tile(Y_train, (X_test_normal.shape[0],1))\n",
    "Dist = np.array([Euclidean, trueLabel])\n",
    "\n",
    "I, J, K = np.ogrid[tuple(map(slice, Dist.shape))]\n",
    "# I, J, K are the identity indices in the sense that (A == A[I, J, K]).all()\n",
    "newK = np.argsort(Dist[0],axis=-1) # first axis of A[1] is second axis of A\n",
    "Dist = Dist[I, J, newK]\n",
    "#a,b = (X_test.shape[0],X_train.shape[0])\n",
    "display(Dist)\n",
    "topK = Dist[1,:,0:k]\n",
    "display(topK)\n",
    "#display(topK)\n",
    "topK = topK.astype(int)\n",
    "display(topK)\n",
    "#display(topK)\n",
    "Y_pred = np.array([])\n",
    "for vector in topK:\n",
    "    counts = np.bincount(vector)\n",
    "    majority = np.argmax(counts)\n",
    "    Y_pred = np.append(Y_pred,majority)\n",
    "display(Y_pred.shape)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_true = Y_test\n",
    "display(accuracy_score(y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.06712859e-02, 1.08226540e-02, 1.10574405e-02, ...,\n",
       "         2.92841730e-01, 2.94245422e-01, 2.97682827e-01],\n",
       "        [7.90492197e-03, 8.26470993e-03, 8.48467208e-03, ...,\n",
       "         4.18210192e-01, 4.18645364e-01, 4.21285074e-01],\n",
       "        [1.17698566e-02, 1.25708989e-02, 1.27096657e-02, ...,\n",
       "         3.01783358e-01, 3.02470321e-01, 3.03002709e-01],\n",
       "        ...,\n",
       "        [1.24217951e-02, 1.28415148e-02, 1.28456472e-02, ...,\n",
       "         2.77452034e-01, 2.79875776e-01, 2.87272537e-01],\n",
       "        [1.14967993e-02, 1.28375296e-02, 1.28740186e-02, ...,\n",
       "         2.35169962e-01, 2.45143444e-01, 2.65554642e-01],\n",
       "        [1.65136654e-02, 1.71756100e-02, 1.76578767e-02, ...,\n",
       "         2.67775242e-01, 2.70404970e-01, 2.78732080e-01]],\n",
       "\n",
       "       [[6.00000000e+00, 9.00000000e+00, 9.00000000e+00, ...,\n",
       "         1.10000000e+01, 1.10000000e+01, 1.10000000e+01],\n",
       "        [3.00000000e+00, 3.00000000e+00, 3.00000000e+00, ...,\n",
       "         1.10000000e+01, 1.10000000e+01, 1.10000000e+01],\n",
       "        [3.00000000e+00, 8.00000000e+00, 3.00000000e+00, ...,\n",
       "         1.10000000e+01, 1.10000000e+01, 1.10000000e+01],\n",
       "        ...,\n",
       "        [9.00000000e+00, 9.00000000e+00, 9.00000000e+00, ...,\n",
       "         1.10000000e+01, 1.10000000e+01, 1.10000000e+01],\n",
       "        [2.00000000e+00, 2.00000000e+00, 2.00000000e+00, ...,\n",
       "         1.10000000e+01, 1.10000000e+01, 1.10000000e+01],\n",
       "        [8.00000000e+00, 8.00000000e+00, 8.00000000e+00, ...,\n",
       "         1.10000000e+01, 1.10000000e+01, 1.10000000e+01]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[6., 9., 9., 6., 6., 6.],\n",
       "       [3., 3., 3., 4., 5., 3.],\n",
       "       [3., 8., 3., 9., 3., 8.],\n",
       "       ...,\n",
       "       [9., 9., 9., 9., 9., 9.],\n",
       "       [2., 2., 2., 2., 2., 2.],\n",
       "       [8., 8., 8., 8., 1., 6.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[6, 9, 9, 6, 6, 6],\n",
       "       [3, 3, 3, 4, 5, 3],\n",
       "       [3, 8, 3, 9, 3, 8],\n",
       "       ...,\n",
       "       [9, 9, 9, 9, 9, 9],\n",
       "       [2, 2, 2, 2, 2, 2],\n",
       "       [8, 8, 8, 8, 1, 6]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6895"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#COSINE GARBAGE ABSOLUTE TRAHS\n",
    "X_train = training[0:8000, 0:48]\n",
    "Y_train = training[0:8000, 48:49]\n",
    "Y_train = Y_train.flatten()\n",
    "X_test = testing[0:2000, 0:48]\n",
    "Y_test = testing[0:2000, 48:49]\n",
    "Y_test = Y_test.flatten()\n",
    "\n",
    "#display(X_train.shape, X_test.shape)\n",
    "X_min = np.min(X_train,axis = 0)\n",
    "X_max = np.max(X_train,axis = 0)\n",
    "X_minMat = np.tile(X_min, (X_train.shape[0],1))\n",
    "X_maxMat = np.tile(X_max, (X_train.shape[0],1))\n",
    "X_train_normal = np.true_divide(X_train-X_minMat, X_maxMat-X_minMat)\n",
    "\n",
    "X_min2 = np.min(X_test,axis = 0)\n",
    "X_max2 = np.max(X_test,axis = 0)\n",
    "X_minMat2 = np.tile(X_min2, (X_test.shape[0],1))\n",
    "X_maxMat2 = np.tile(X_max2, (X_test.shape[0],1))\n",
    "X_test_normal = np.true_divide(X_test-X_minMat2, X_maxMat2-X_minMat2)\n",
    "#display(X_train_normal.shape, X_test_normal.shape)\n",
    "\n",
    "X_train_L1norm = np.apply_along_axis(np.linalg.norm, 1, X_train_normal)\n",
    "X_test_L1norm = np.apply_along_axis(np.linalg.norm, 1, X_test_normal)\n",
    "#display(X_train_L1norm.shape, X_test_L1norm.shape)\n",
    "\n",
    "C = np.transpose(np.tile(X_train_L1norm, (X_train.shape[1],1)))\n",
    "D = np.transpose(np.tile(X_test_L1norm, (X_test.shape[1],1)))\n",
    "#display(C.shape,D.shape)\n",
    "\n",
    "X_train_final = np.true_divide(X_train_normal,C)\n",
    "X_test_final = np.true_divide(X_test_normal,D)\n",
    "\n",
    "X_train_final = np.transpose(X_train_final)\n",
    "#display(X_test_final.shape, X_train_final.shape)\n",
    "\n",
    "dist = np.dot(X_test_final, X_train_final)\n",
    "k = 6\n",
    "trueLabel = np.tile(Y_train, (X_test_final.shape[0],1))\n",
    "ones = np.ones((X_test.shape[0],X_train.shape[0]))\n",
    "Dist = np.subtract(ones,dist)\n",
    "Dist = np.array([Dist, trueLabel])\n",
    "\n",
    "I, J, K = np.ogrid[tuple(map(slice, Dist.shape))]\n",
    "# I, J, K are the identity indices in the sense that (A == A[I, J, K]).all()\n",
    "newK = np.argsort(Dist[0],axis=-1) # first axis of A[1] is second axis of A\n",
    "Dist = Dist[I, J, newK]\n",
    "#a,b = (X_test.shape[0],X_train.shape[0])\n",
    "display(Dist)\n",
    "topK = Dist[1,:,0:k]\n",
    "display(topK)\n",
    "#display(topK)\n",
    "topK = topK.astype(int)\n",
    "display(topK)\n",
    "#display(topK)\n",
    "Y_pred = np.array([])\n",
    "for vector in topK:\n",
    "    counts = np.bincount(vector)\n",
    "    majority = np.argmax(counts)\n",
    "    Y_pred = np.append(Y_pred,majority)\n",
    "display(Y_pred.shape)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_true = Y_test\n",
    "display(accuracy_score(y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = Dist[:,0:k,:]\n",
    "#display(topK)\n",
    "topK = topK[:,:,1].astype(int)\n",
    "#display(topK)\n",
    "Y_pred = np.array([])\n",
    "for vector in topK:\n",
    "    counts = np.bincount(vector)\n",
    "    majority = np.argmax(counts)\n",
    "    Y_pred = np.append(Y_pred,majority)\n",
    "#display(Y_pred)\n",
    "#=sortDist = np.sort(sortDist, axis = 2)\n",
    "#display(sortDist)\n",
    "#sortDist = sorted(data, key=lambda x: x[1])\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_true = Y_test\n",
    "display(y_true.shape)\n",
    "display(Y_pred.shape)\n",
    "display(accuracy_score(y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train,X_test,Y_train, k):\n",
    "    y_pred = np.array([])\n",
    "    for point in X_test:\n",
    "        dist = np.array([])\n",
    "        for row in X_train:\n",
    "            d = np.sqrt(np.sum((point-row)**2))\n",
    "            dist = np.append(dist, d)\n",
    "        attach = np.vstack((dist, Y_train))\n",
    "        sortDist = attach[:,attach[0].argsort()]\n",
    "        topK = sortDist[:,0:k]\n",
    "        topK = topK[1].astype(int)\n",
    "        counts = np.bincount(topK)\n",
    "        majority = np.argmax(counts)\n",
    "        y_pred = np.append(y_pred,majority)\n",
    "    return y_pred\n",
    "        \n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    ##NP.tile np.repeat\n",
    "    ##np.lin(norx)\n",
    "    ##creating a matrix with same test row repeating operation on entire train matrix\n",
    "    ##(A-B)^2 = A^2 - 2AB + B^2\n",
    "    ##cosine distance also an option\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([])\n",
    "d = np.append(d, 2)\n",
    "np.append(d, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train,Y_train,X_test):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5.5, 6.5, 7.5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-4.5, -4.5, -4.5],\n",
       "       [-1.5, -1.5, -1.5],\n",
       "       [ 1.5,  1.5,  1.5],\n",
       "       [ 4.5,  4.5,  4.5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Demo of centering data (mean = 0)\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "display(A)\n",
    "\n",
    "mean = A.mean(axis=0)\n",
    "display(mean)\n",
    "display(mean.shape)\n",
    "\n",
    "new_A = A - mean\n",
    "# Or use newaxis:\n",
    "# new_A = A - mean[np.newaxis, :]\n",
    "\n",
    "display(new_A)\n",
    "\n",
    "display(A.mean())\n",
    "display(new_A.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.5, -4.5, -4.5],\n",
       "       [-1.5, -1.5, -1.5],\n",
       "       [ 1.5,  1.5,  1.5],\n",
       "       [ 4.5,  4.5,  4.5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([3.35410197, 3.35410197, 3.35410197])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.34164079, -1.34164079, -1.34164079],\n",
       "       [-0.4472136 , -0.4472136 , -0.4472136 ],\n",
       "       [ 0.4472136 ,  0.4472136 ,  0.4472136 ],\n",
       "       [ 1.34164079,  1.34164079,  1.34164079]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Demo of standardizing data (s.d. = 1)\n",
    "\n",
    "display(new_A)\n",
    "\n",
    "new_sd = np.std(new_A, axis=0)\n",
    "display(new_sd)\n",
    "\n",
    "newer_A = new_A / new_sd\n",
    "display(newer_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [2, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2., -2.],\n",
       "       [-2.,  2.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Covariance matrix demo\n",
    "\n",
    "Z = np.array([[0, 2], [2, 0]])\n",
    "display(Z)\n",
    "cov = np.cov(Z)\n",
    "display(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 2.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678, -0.31622777],\n",
       "       [ 0.70710678,  0.9486833 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.70710678, 0.70710678])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([4.24264069, 4.24264069])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Demo of eigendecomposition\n",
    "\n",
    "A = np.array([[5, 1],[3, 3]])\n",
    "\n",
    "e_vals, e_vects = np.linalg.eig(A)\n",
    "\n",
    "display(e_vals)\n",
    "display(e_vects)\n",
    "\n",
    "display(e_vals[0])\n",
    "display(e_vects[:, 0])\n",
    "e_recomp = e_vals[0] * e_vects[:, 0]\n",
    "display(e_recomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 8, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[4, 8, 7],\n",
       "       [9, 5, 1],\n",
       "       [2, 2, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([8, 4, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[8, 7, 4],\n",
       "       [5, 1, 9],\n",
       "       [2, 5, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sorting eigenvectors based on eigenvalues\n",
    "\n",
    "eigenValues = np.array([1, 8, 4])\n",
    "eigenVectors = np.array([[4, 8, 7], [9, 5, 1], [2, 2, 5]])\n",
    "\n",
    "display(eigenValues)\n",
    "display(eigenVectors)\n",
    "\n",
    "idx = eigenValues.argsort()[::-1]   \n",
    "eigenValues = eigenValues[idx]\n",
    "eigenVectors = eigenVectors[:,idx]\n",
    "\n",
    "display(eigenValues)\n",
    "display(eigenVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [2, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [2, 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calculate Z* = ZP*\n",
    "\n",
    "# Z_star = Z\n",
    "\n",
    "a = np.array([[1, 0], \n",
    "              [0, 1]])\n",
    "\n",
    "b = np.array([[4, 1], \n",
    "              [2, 2]])\n",
    "\n",
    "display(np.dot(a, b))\n",
    "display(a @ b)\n",
    "\n",
    "# array([[4, 1],\n",
    "#        [2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [4],\n",
       "       [7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the top N features to keep\n",
    "\n",
    "M = np.array([[1, 2, 3], \n",
    "              [4, 5, 6], \n",
    "              [7, 8, 9]])\n",
    "topN = M[:, :N]\n",
    "display(topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4055e-05,  4.4589e-05,  9.4860e-05, -5.2550e-06,  1.0533e-04],\n",
       "       [ 5.4193e-06, -1.0994e-05, -6.0426e-05,  3.2712e-06, -3.3251e-06],\n",
       "       [-1.2952e-05, -1.6223e-05,  2.7494e-04, -1.2440e-05, -5.8102e-05],\n",
       "       [-1.0194e-05, -3.0570e-06, -2.6091e-04,  1.3139e-06, -8.2072e-06],\n",
       "       [-1.4395e-05,  7.2051e-05, -1.3032e-04, -1.2995e-05,  5.1771e-05]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.766834e-05,  2.731580e-05,  1.112312e-04, -3.402000e-08,\n",
       "         8.783666e-05],\n",
       "       [ 9.032640e-06, -2.826720e-05, -4.405480e-05,  8.492180e-06,\n",
       "        -2.081844e-05],\n",
       "       [-9.338660e-06, -3.349620e-05,  2.913112e-04, -7.219020e-06,\n",
       "        -7.559534e-05],\n",
       "       [-6.580660e-06, -2.033020e-05, -2.445388e-04,  6.534880e-06,\n",
       "        -2.570054e-05],\n",
       "       [-1.078166e-05,  5.477780e-05, -1.139488e-04, -7.774020e-06,\n",
       "         3.427766e-05]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.56104295,  0.78326296,  0.59978997, -0.00504487,  1.56757145],\n",
       "       [ 0.79805681, -0.81054375, -0.2375559 ,  1.25931726, -0.37153499],\n",
       "       [-0.82509446, -0.96048195,  1.57083208, -1.07051858, -1.3491075 ],\n",
       "       [-0.58141811, -0.58295538, -1.31862212,  0.9690665 , -0.45866308],\n",
       "       [-0.95258719,  1.57071812, -0.61444404, -1.15282031,  0.61173411]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.56104295,  0.78326296,  0.59978997, -0.00504487,  1.56757145],\n",
       "       [ 0.79805681, -0.81054375, -0.2375559 ,  1.25931726, -0.37153499],\n",
       "       [-0.82509446, -0.96048195,  1.57083208, -1.07051858, -1.3491075 ],\n",
       "       [-0.58141811, -0.58295538, -1.31862212,  0.9690665 , -0.45866308],\n",
       "       [-0.95258719,  1.57071812, -0.61444404, -1.15282031,  0.61173411]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.45137012, -0.17378062, -0.20828692, -0.27526316,  0.20596059],\n",
       "       [-0.17378062,  0.74822113, -0.19099838,  0.49103103, -0.87447316],\n",
       "       [-0.20828692, -0.19099838,  1.41223486, -0.62240413, -0.39054542],\n",
       "       [-0.27526316,  0.49103103, -0.62240413,  0.6969715 , -0.29033524],\n",
       "       [ 0.20596059, -0.87447316, -0.39054542, -0.29033524,  1.34939323]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2.23082022e+00, 1.87471612e+00, 5.38363278e-01, 1.16360450e-16,\n",
       "       1.42912254e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.19218966,  0.07408832,  0.81470931, -0.4472136 , -0.30630506],\n",
       "       [ 0.56564357,  0.09970042,  0.13185132, -0.4472136 ,  0.67284649],\n",
       "       [-0.01099026, -0.86014638, -0.2144233 , -0.4472136 , -0.11853295],\n",
       "       [ 0.35602963,  0.41091189, -0.41543504, -0.4472136 , -0.57602782],\n",
       "       [-0.71849328,  0.27544576, -0.31670229, -0.4472136 ,  0.32801935]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2.23082022e+00, 1.87471612e+00, 5.38363278e-01, 1.16360450e-16,\n",
       "       1.42912254e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.19218966,  0.07408832,  0.81470931, -0.4472136 , -0.30630506],\n",
       "       [ 0.56564357,  0.09970042,  0.13185132, -0.4472136 ,  0.67284649],\n",
       "       [-0.01099026, -0.86014638, -0.2144233 , -0.4472136 , -0.11853295],\n",
       "       [ 0.35602963,  0.41091189, -0.41543504, -0.4472136 , -0.57602782],\n",
       "       [-0.71849328,  0.27544576, -0.31670229, -0.4472136 ,  0.32801935]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2.23082022e+00, 1.87471612e+00, 5.38363278e-01, 1.42912254e-02,\n",
       "       1.16360450e-16])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.19218966,  0.07408832,  0.81470931, -0.30630506, -0.4472136 ],\n",
       "       [ 0.56564357,  0.09970042,  0.13185132,  0.67284649, -0.4472136 ],\n",
       "       [-0.01099026, -0.86014638, -0.2144233 , -0.11853295, -0.4472136 ],\n",
       "       [ 0.35602963,  0.41091189, -0.41543504, -0.57602782, -0.4472136 ],\n",
       "       [-0.71849328,  0.27544576, -0.31670229,  0.32801935, -0.4472136 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.99164618,  0.10754743,  0.75210389,  0.49486525, -2.01542284],\n",
       "       [ 0.10605332,  0.59777868,  0.18875204, -1.6089346 , -0.28520575],\n",
       "       [ 0.18620867, -2.3195306 , -0.26367958, -0.40560654,  1.17812626],\n",
       "       [ 0.47105231,  1.30487501, -0.52513141, -0.76650763,  0.88217005],\n",
       "       [ 0.22833188,  0.30932949, -0.15204493,  2.28618352,  0.24033228]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.99164618,  0.10754743],\n",
       "       [ 0.10605332,  0.59777868],\n",
       "       [ 0.18620867, -2.3195306 ],\n",
       "       [ 0.47105231,  1.30487501],\n",
       "       [ 0.22833188,  0.30932949]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PRACTICE\n",
    "\n",
    "X_train = training[0:5, 0:5]\n",
    "display(X_train)\n",
    "N = 2\n",
    "\n",
    "#1) Center the data (mean=0) and standardize (standard deviation = 1) (call matrix Z)\n",
    "#Center data\n",
    "mean = X_train.mean(axis=0)\n",
    "# display(mean)\n",
    "# display(mean.shape)\n",
    "# centered_X = X_train - mean\n",
    "# Or use newaxis:\n",
    "center_X = X_train - mean[np.newaxis, :]\n",
    "display(center_X)\n",
    "# display(X_train.mean())\n",
    "# display(center_X.mean())\n",
    "\n",
    "#Standardize data\n",
    "sd = np.std(center_X, axis=0)\n",
    "# display(sd)\n",
    "Z = center_X / sd\n",
    "display(Z)\n",
    "\n",
    "#2) Find the covariance matrix of Z (Zt*Z)\n",
    "display(Z)\n",
    "cov = np.cov(Z)\n",
    "display(cov)\n",
    "\n",
    "#3) Using eigendecomposition, decompose (Zt*Z) into PDP^-1\n",
    "e_vals, e_vects = np.linalg.eig(cov)\n",
    "\n",
    "display(e_vals)\n",
    "display(e_vects)\n",
    "\n",
    "#4) Sort the eigenvectors by their corresponding eigenvalues from largest to smallest (call new matrix P*)\n",
    "display(e_vals)\n",
    "display(e_vects)\n",
    "\n",
    "idx = e_vals.argsort()[::-1]   \n",
    "e_vals = e_vals[idx]\n",
    "P_star = e_vects[:,idx]\n",
    "\n",
    "display(e_vals)\n",
    "display(P_star)\n",
    "\n",
    "#5) Calculate Z* = ZP* (new matrix Z* is a centered/standardized version of X's principle components)\n",
    "Z_star = Z @ P_star\n",
    "display(Z_star)\n",
    "\n",
    "#6) Determine the top N features to keep\n",
    "topN = Z_star[:, :N]\n",
    "display(topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X_train,N):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type N: int\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    #1) Center the data (mean=0) and standardize (standard deviation = 1) (call matrix Z)\n",
    "    #Center the data\n",
    "    mean = X_train.mean(axis=0)\n",
    "    center_X = X_train - mean[np.newaxis, :]\n",
    "#     display(center_X)    \n",
    "    \n",
    "    #Standardize the data\n",
    "    sd = np.std(center_X, axis=0)\n",
    "    Z = center_X / sd\n",
    "#     display(Z)\n",
    "    \n",
    "    #2) Find the covariance matrix of Z (Zt*Z)\n",
    "    cov = np.cov(Z)\n",
    "#     display(cov)\n",
    "    \n",
    "    #3) Using eigendecomposition, decompose (Zt*Z) into PDP^-1\n",
    "    #Where P = matrix of eigenvectors, D = vector of eigenvalues which corresponds to P\n",
    "    e_vals, e_vects = np.linalg.eig(cov)\n",
    "#     display(e_vals)\n",
    "#     display(e_vects)\n",
    "    \n",
    "    #4) Sort the eigenvectors by their corresponding eigenvalues from largest to smallest (call new matrix P*)\n",
    "    #May be unnecessary is eigenvalues/vectors are return in descending order\n",
    "    idx = e_vals.argsort()[::-1]   \n",
    "    e_vals = e_vals[idx]\n",
    "    P_star = e_vects[:,idx]\n",
    "\n",
    "#     display(e_vals)\n",
    "#     display(P_star)\n",
    "    \n",
    "    #5) Calculate Z* = ZP* (new matrix Z* is a centered/standardized version of X's principle components)\n",
    "    #Eigenvectors are the directions, Eigenvalues are the magnitudes/importance\n",
    "    Z_star = Z @ P_star\n",
    "    \n",
    "    #6) Determine the top N features to keep\n",
    "    topN = Z_star[:, :N]\n",
    "#     display(topN)\n",
    "    return topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify acccuracy with sklearn PCA\n",
    "\n",
    "X_train = training[0:5, 0:5]\n",
    "# display(X_train)\n",
    "N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.04725082e-04,  9.89889464e-05],\n",
       "       [-4.15586840e-05, -3.41273120e-05],\n",
       "       [ 2.96414581e-04, -6.18740550e-05],\n",
       "       [-2.41627047e-04, -5.01569289e-05],\n",
       "       [-1.17953931e-04,  4.71693496e-05]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA as scikitPCA\n",
    "\n",
    "#Center data\n",
    "# mean = X_train.mean(axis=0)\n",
    "# center_X = X_train - mean[np.newaxis, :]\n",
    "\n",
    "# #Standardize data\n",
    "# sd = np.std(center_X, axis=0)\n",
    "# Z = center_X / sd\n",
    "# # display(Z)\n",
    "\n",
    "Z = X_train\n",
    "\n",
    "pca = scikitPCA(n_components=N)\n",
    "pca.fit(Z)\n",
    "\n",
    "reduced_dim = pca.transform(Z)\n",
    "\n",
    "display(reduced_dim)\n",
    "\n",
    "display(X_train.shape)\n",
    "display(reduced_dim.shape)\n",
    "\n",
    "# display(pca.explained_variance_ratio_)\n",
    "# display(pca.singular_values_)\n",
    "# display(pca.get_covariance())\n",
    "# display(pca.get_params())\n",
    "\n",
    "# display(np.transpose(pca.components_))\n",
    "# display(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99164618,  0.10754743],\n",
       "       [ 0.10605332,  0.59777868],\n",
       "       [ 0.18620867, -2.3195306 ],\n",
       "       [ 0.47105231,  1.30487501],\n",
       "       [ 0.22833188,  0.30932949]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#My PCA Implementation\n",
    "\n",
    "topNFeatures = PCA(X_train, N)\n",
    "display(topNFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(reduced_dim, topNFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans(X_train,N):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type N: int\n",
    "    :rtype: List[numpy.ndarray]\n",
    "    \"\"\"\n",
    "    \n",
    "    #1) Choose the number of clusters (K = N)\n",
    "    \n",
    "    #2) Initialize N centroids to \"random\" values of the data,\n",
    "    #Either choose them at random: (Find the min and max of the data, then calculate random coordinates from there)\n",
    "    #Or sort the dataset, split it into K portions and pick one datapoint from each portion as a centriod.\n",
    "    \n",
    "    #3) Reassign all points to the closest centroid\n",
    "    #Calculate Euclidean distance between each point and each centroid, assign each point to the min dist centroid\n",
    "    \n",
    "    #4) Recalculate centroid coordinate based on point assignment\n",
    "    #New centroid value is the average of all point that currently have that centroid's label\n",
    "    \n",
    "    #5) Repeat steps 3 and 4 until there is no change in centroid coordinates, or points stop switching clusters\n",
    "    #Either keep track and only stop is all of the centroids don't change values\n",
    "    #Or only repeat a fixed number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SklearnSupervisedLearning(X_train,Y_train,X_test):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: List[numpy.ndarray] \n",
    "    \"\"\"\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SklearnVotingClassifier(X_train,Y_train,X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: List[numpy.ndarray] \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create your own custom functions for Matplotlib visualization of hyperparameter search. \n",
    "Make sure that plots are labeled and proper legends are used\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
